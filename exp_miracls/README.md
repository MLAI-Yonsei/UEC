# Multilingual Evaluation (MIRACL Subset)

This document provides instructions on how to run the multilingual evaluation experiments using the MIRACL dataset. The process consists of three main steps:

1.  **Probabilistic Embedding Generation (`fit_la.py`)**: Convert deterministic embeddings into probabilistic ones using Laplace Approximation.
2.  **Baseline Performance Evaluation (`run_baseline.py`)**: Measure the performance of individual models and standard ensemble methods (uniform/weighted).
3.  **UEC Performance Evaluation (`run_uec.py`)**: Evaluate the performance of the Uncertainty-driven Embedding Convolution (UEC) method.

---

### 1. Probabilistic Embedding Generation (`fit_la.py`)

This script fits a Laplace Approximation to a pre-trained sentence embedding model using data from the MIRACL dataset. This process generates and saves a probabilistic version of the model, which captures uncertainty in the embeddings.

**Example Usage:**

```bash
python exp_miracls/fit_la.py \
    --model_name "intfloat/e5-base-v2" \
    --save_path_template "/path/to/your/bem/la_models_miracls/{model_folder_name}/{backend}" \
    --languages "en" "ru" "zh" "ar" \
    --num_samples_per_lang 25 \
    --backend AsdlEF \
    --evaluation
```

**Key Arguments:**

*   `--model_name`: The HuggingFace identifier for the pre-trained model you want to use (e.g., `"intfloat/e5-base-v2"`).
*   `--save_path_template`: A template for the directory where the fitted Laplace model will be saved. The placeholders `{model_folder_name}` and `{backend}` will be automatically filled. **You must change `/path/to/your/bem/` to your actual path.**
*   `--languages`: A list of languages from the MIRACL dataset to use for fitting the approximation.
*   `--num_samples_per_lang`: The number of query-passage pairs to use from each specified language.
*   `--backend`: The backend library to use for Hessian computation (e.g., `AsdlEF`).
*   `--evaluation`: A flag that, if present, triggers an evaluation of the model's performance after the fitting process is complete.

---

### 2. Baseline Performance Evaluation (`run_baseline.py`)

This script evaluates the retrieval performance of different models and ensemble techniques on the MIRACL dev set. It serves as a baseline for comparison with the UEC method.

**Evaluation Modes:**

*   **`individual`**: Evaluates each model specified in the script separately.
*   **`baseline`**: Evaluates a single, strong baseline model (e.g., `Alibaba-NLP/gte-modernbert-base`).
*   **`uniform`**: Evaluates an ensemble where the embeddings of all models are averaged with uniform weights.
*   **`weighted`**: Performs a grid search to find optimal weights for the ensemble and then evaluates it.

**Example Usage:**

```bash
python exp_miracls/run_baseline.py \
    --languages "en" "ru" "zh" "ar" \
    --max_dataset_samples 25 \
    --eval_modes "individual" "baseline" "uniform" "weighted" \
    --embedding_cache_file "miracl_baseline_cache.pt"
```

**Key Arguments:**

*   `--languages`: A list of languages from the MIRACL dev set to use for evaluation.
*   `--max_dataset_samples`: The maximum number of samples to use from each language's dev set.
*   `--eval_modes`: A list of evaluation modes to run. You can specify one or more from the options above.
*   `--embedding_cache_file`: The path to a file where computed embeddings will be cached. This speeds up subsequent runs by avoiding re-computation.

> **Note**: The models used in this script (e.g., for `individual` and `uniform`/`weighted` modes) are currently hardcoded within `run_baseline.py`. You may need to modify the script directly to change the set of models.

---

### 3. UEC Performance Evaluation (`run_uec.py`)

This script evaluates the performance of the Uncertainty-driven Embedding Convolution (UEC) method. It loads the probabilistic models generated by `fit_la.py`, combines their embeddings using uncertainty-aware weights, and evaluates the final retrieval performance.

**Example Usage:**

```bash
python exp_miracls/run_uec.py \
    --model_names "intfloat/e5-base-v2" "BAAI/bge-base-zh-v1.5" "silma-ai/silma-embedding-matryoshka-v0.1" \
    --model_paths "/path/to/your/bem/la_models_miracls/e5-base-v2/AsdlEF" \
                  "/path/to/your/bem/la_models_miracls/bge-base-zh-v1.5/AsdlEF" \
                  "/path/to/your/bem/la_models_miracls/silma-embedding-matryoshka-v0.1/AsdlEF" \
    --scales 0.01 0.0001 100.0 \
    --languages "en" "ru" "zh" "ar" \
    --max_dataset_samples 25 \
    --embedding_cache_file "miracl_uec_cache.pt" \
    --beta 0.1 \
    --temperature 1.0
```

**Key Arguments:**

*   `--model_names`: A list of HuggingFace identifiers for the models to be ensembled.
*   `--model_paths`: A list of paths to the directories where the corresponding probabilistic models (from `fit_la.py`) are saved. **This must be updated to your actual paths.** The order must match `--model_names`.
*   `--scales`: A list of scaling factors for each model in the UEC convolution. The order must match `--model_names`.
*   `--languages`: A list of languages from the MIRACL dev set to evaluate on.
*   `--max_dataset_samples`: The maximum number of samples to use from each language's dev set.
*   `--embedding_cache_file`: The path to a file for caching embeddings to speed up subsequent runs.
*   `--beta`: The beta parameter for the uncertainty-aware cosine similarity function.
*   `--temperature`: A temperature scaling factor for the UEC weighting mechanism. It controls the sharpness of the distribution over the models.

